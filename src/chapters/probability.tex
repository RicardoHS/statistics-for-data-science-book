This chapter will focus on the main probabilistic knowledge necessary to have a strong mathematical base. One of the key thing to know about
probabilities is that all the theory is builded from the set theory. Because of that the very first points to take into account will be the
fundamental set aspects. After that the chapter will go inside little by little into the probabilistic theory.

\section{Random Experiments}
 A experiment is deterministic when knowing the state of the set of variables involved in the experiment the outcome is always the same.
 Conversely a experiment is random when knowing the state (or when we ignore some of these states) of the set of variables involved in
 the experiment the outcome differs. \\
 
 In other terms, \textbf{an experiment is random} if although it is repeated in the same manner every time, can result in different outcomes.
 The probability theory study the random experiments.\\

 The elements always involved in a random experiment are the followings:
 \begin{itemize}
     \item \textbf{Sample Space}, is the set of all possible outcomes of the random experiment. It's denoted by $S$
     \item \textbf{Event}, is a subset of $S$. Any possible outcome of the random experiment. 
     \item \textbf{Null Event}, is a special event that never occurs. Denoted by $\emptyset$
 \end{itemize}

 \subsection{Set Operations}
 Also there is a set of \textbf{operations}:
 \begin{itemize}
     \item \textbf{Union}, occurs when either of the two events (or both of them simultaneously) do occur. Denoted $A \cup B$ (Grammatically $A$ or $B$)
     \item \textbf{Intersection}, occurs when both of them do simultaneously occur. Denoted $A \cap B$ (Grammatically $A$ and $B$)
     \item \textbf{Complementary}, occurs when the event does not occur. Denoted $\overline{A}$ (Grammatically not $A$)
     \item \textbf{Difference}, occurs when the first event does occurs, but the second does not. Denoted $A \setminus B$ (Grammatically $A$ and not $B$).
        Note that $A \setminus B = A \cap \overline{B}$
 \end{itemize}

\subsection{Set Algebra}
Derived from above there is some general properties.
\begin{itemize}
    \item Commutative
    \item Associative
    \item Neutral Element
    \item Complementation
    \item Idempotence
    \item Abosortion
    \item Simplication
\end{itemize}

And also some distributive laws
\begin{itemize}
    \item Union with Intersection, $(A \cap B)\cup C = (A\cup C) \cap (B \cup C) $
    \item Intersection with Union, $(A \cup B)\cap C = (A\cap C) \cup (B \cup C) $
\end{itemize}

De Morgan's laws
\begin{itemize}
    \item $\overline{A \cup B} = \overline{A} \cap \overline{B}$
    \item $\overline{A \cap B} = \overline{A} \cup \overline{B}$
\end{itemize}

\section{Probability}
A probability $P$ assesses a number to every event $A$ associated with the random experiment. It is interpreted as the likelihood (or chance) that $A$ occurs.

\subsection{Axioms}
The Kolmogorov axioms are a fundamental part of probability theory defined by Andrey Kolmogorov. In it, the probability $P$ of some event $E$, denoted $P(E)$ 
is usually defined as to satisfy these axioms. 
\begin{itemize}
    \item \textbf{Non Negativity}: $P(A) \geq 0$ for every $A$
    \item \textbf{Additivity}: Any countable sequence of disjoint sets $A_i$ satisfies $P(\bigcup_{i \in I} A_i) = \sum_{i \in I} P(A_i) $
    \item \textbf{Normalization}: $P(S)=1$
\end{itemize}

The structure of the family of events at with a probability is defined is known as $\sigma$-algebra (sigma-algebra).
A $\sigma$-algebra 
\begin{enumerate}
    \item contains the certain event $S$
    \item is closed under complementation
    \item is closed under countable unions.
\end{enumerate}

Write here text about family set explanation

\subsection{Properties}
\begin{itemize}
    \item P
    \item P
    \item P
    \item P
    \item P
    \item P
\end{itemize}

\subsection{LaPlace's Rule}
bla bla

\subsection{Conditional Probability}
\begin{center}
    $P(A|B) = \frac{P(A\cap B)}{P(B)}$
\end{center}

\subsubsection{Properties}

\subsection{Bayes Formula}
One event form $P(A \cap B) = P(A)P(B|A)$\\
general form .....

\subsubsection{Multiplication Rule}
bla bla

\subsubsection{Total Probability Rule}
bla bla

\subsection{Independency of two events}
bla bla

\subsubsection{Conditional Independece}
bla blo

\subsection{Combinatory Logic}
Also know as expertise in counting \\
bla bla

