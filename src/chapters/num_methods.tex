This chapter will focus on the methods and processes needed to choose the optimal 
solution given a mathematical model. This chapter also include the processes needed to build these models.\\

This field of study is also know as \textbf{Prescriptive Analysis} or \textbf{Operations Research}.

The schematic way to proceed in this problems is the following:
\begin{center}
    Problem Description $\rightarrow$ Model Formulation $\rightarrow$ Analysis \& Algorithms $\rightarrow$
    Computer Solution $\rightarrow$ Interpretation
\end{center}

\section{Notation}
\subsection{Elements}
The elements involved in all mathematical decision models are the following:
\begin{itemize}
    \item Decision Variables: Are the ones we want to know his optimal value. (e.g. number of products to 
    build in a period of time)
        \[ x = (x_1, x_2, \dots , x_n) \]
    \item Objective Function: Is the one that model the problem. It will be always needed to \textit{maximize} or \textit{minimize} it.
    \begin{center}
        $\text{maximize } f(x)$ or $\text{minimze } f(x)$
    \end{center}
    \item Functional Constraints (or structural constrains): Are the limitations to the objective function and is a function of all the variables.
        \[ a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n \leq b_1 \]
    \item Non-negativity Constraints: Are the limitations to the objective function.
        \[ g_1(x)\leq b_1,\;\; g_2(x)\geq b_2, \;\; b_3=b_3 \]
\end{itemize}

The main goal to a correct model building is to identify these elements and define it in a proper way. Contrary that the logic 
intuition could make us think, simplest models are better than complex one with lot of constraints and elaborated objective functions. \\

\textbf{All models are wrong, but some are useful} (? citation)\\

Why use this models? Because most of the times the time and memory computational complexity to resolve some problems in a brute force
or in a non analytical way is way greater than the models developed using this approach.

\subsection{Standard Form of the Model}
A problem is described in the standard form if it is described in the following way:\\
\[ \text{maximize } c_1x_1 + c_2x_2 + \dots + c_nx_n\]
Subject to the restrictions
\begin{gather*}
    a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n \leq b_1 \\
    a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n \leq b_2 \\
    \dots\\
    a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n \leq b_m
\end{gather*}
and
\[ x_1 \geq 0,\;\;\; x_2 \geq 0,\;\;\; \dots\;\;\; x_n \geq 0 \]

\subsection{Other Forms}
\begin{itemize}
    \item Minimizing rather than maximizing the objective function.
    \[ \text{minimize } c_1x_1 + c_2x_2 + \dots + c_nx_n\]
    \item Functional constrains with a greather-or-equal inequality
    \[ a_{i1}x_1 + a_{i2}x_2 + \dots + a_{in}x_n \geq b_i \]
    \item Functional constrains in equation form
    \[ a_{i1}x_1 + a_{i2}x_2 + \dots + a_{in}x_n = b_i \]
    \item Deleting the non-negativity constrains
    \[ x_j \text{is unrestricted in sign for some values of } j \]
\end{itemize}

\subsection{Terminology for solutions of the model}
\begin{itemize}
    \item \textbf{Feasible solution} is a solution that satisfies all the constrains. It is possible for a problem to have no feasible solutions.
    \item \textbf{Infeasible solution} is a solution with at least one constrains unsatisfied.
    \item \textbf{Feasible region} is the set of possibles solutions of the model. It's always a convex polytope. This polytope is defined by the intersection of the problem constraints
    \item \textbf{Optimal solution} of the problem. Is whose that \textit{maximize} or \textit{minimize} the objective function inside the feasible region. 
    Usually, this optimal solution is find computationally. It is possible for a problem to have multiples optimal solutions, also is possible to do not have them.
    \item A \textbf{Corner-point feasible (CPF)} solution is a solution that lies at a corner of the feasible region.
\end{itemize}

\subsubsection{Realtionship between optimal solutions and CPF solutions}
Consider any linear pro-gramming problem with feasible solutions and a bounded feasible region. The problem
must possess CPF solutions and at least one optimal solution. Furthermore, the best CPF
solution must be an optimal solution. Thus, if a problem has exactly one optimal solution,
it must be a CPF solution. If the problem has multiple optimal solutions, at least two must
be CPF solutions.


\section{Linear Optimization (LO) Models}
Most important type of decision optimization models. Is the foundation to understand the complex ones. Also is one of the most widely
applied models because of his simplicity.\\

\subsubsection{Limitations}
\begin{itemize}
    \item Decision variables needs to be continuous. 
        \[x = (x_1, x_2, \dots , x_n) \in \mathbb{R}\]
    \item Objective function needs to be linear in $x$. Called \textit{}.
        \[ f(x_1,x_2, \dots, x_n) = c_1x_1 + c_2x_2 + \dots + c_nx_n \]
    \item Constraints needs to be linear in $x$
        \begin{gather*}
            a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n \leq b_1 \\
            a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n \leq b_2 \\
            \dots\\
            a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n \leq b_m
        \end{gather*}
    \item Decision Variables needs to be non-negative
        \[ x_1 \geq 0,\;\;\; x_2 \geq 0,\;\;\; \dots\;\;\; x_n \geq 0 \]
\end{itemize}

An equation in a form called \textbf{Slope-intercept form} is the one in the form $x_n = ax_i + bZ,\;\; \forall a,b \in \mathbb{R}$ and demonstrate that the slope of the line is $a$. This means that an increase
of one value in $x_n$ implies an increment of $a$ in $x_i$. Whereas, the intercept of the line with the $x_n$ axis is $bZ$.\\

\subsubsection{Assumptions of a LO problem}
\begin{itemize}
    \item Proportionally, realted with linearity of the objective functions and constrains.
    \item Additivity, Every function is the sum of the individual contributions of the respectives activities.
    \item Divisibility, decision variables are in $\mathbb{R}$
    \item Certainty, the value assigned to each parameter is assumed to be a known constant. This allow us to perform sensitivity analysis.
\end{itemize}

\subsection{Duality}
Every linear programming problem has associated with it another linear programming
problem called the \textbf{dual}.\\

Wikipedia\\
The duality principle is the principle that optimization problems may be viewed from either of two perspectives, the \textbf{primal problem} or the \textbf{dual problem}. 
The solution to the dual problem provides a lower bound to the solution of the primal (minimization) problem. However in general the optimal values of 
the primal and dual problems need not be equal. Their difference is called the duality gap. For convex optimization problems, the duality gap is zero under 
a constraint qualification condition. \\

These conditions are:
\begin{itemize}
    \item Non-negative variables
    \[x = (x_1^+, x_2^+, \dots , x_n^+) \in \mathbb{R}\]
    \item The objective functions need to be maximize
    \[ \text{maximize } c_1x_1 + c_2x_2 + \dots + c_nx_n\]
    \item All constrains must be equality (non inecuations)
    \[ a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n = b_m \]
    \item The right-hand side values must be non-negatives
    \[ b_m > 0,\;\; \forall m \in \{1,2,\dots,m\} \]
\end{itemize}

If some of this conditions are not satisfied, we can convert the equations to make it satisfied.\\
\textbf{For constrains that are non-equality}\\
Given, $x_1 \leq 4$ we can convert it to a new variable. Just,
\[ x_1 \leq 4 \equiv  x_3 = 4 - x_1\]
This new variable $x_3$ is called \textbf{slack or surplus variable} depending of what is doing. So for a problem,
\setlength{\columnseprule}{1pt}
\begin{multicols}{2}
    Original Problem \begin{multline*} 
        \text{maximize } Z = 3x_1 + 5x_2 \\
        \text{subject to }\\
        x_1 \leq 4\\
        2x_2 \leq 12
    \end{multline*}

    Augmented Problem \begin{multline*}
        \text{maximize } Z = 3x_1 + 5x_2 \\
        \text{subject to }\\
        x_1 + x_3 = 4\\
        2x_2 + x_4 = 12
    \end{multline*}
\end{multicols}
\textbf{For variables with unrestricted sign}\\
TODO

\subsubsection{Formulation}
\setlength{\columnseprule}{1pt}
\begin{multicols}{2}
    Primal Problem \begin{multline*} 
        \text{maximize } Z = \sum_{j=1}^n c_jx_j \\
        \text{subject to }\\
        \sum_{j=1}^n a_{ij}x_j \leq b_i, \;\;\; \forall i \in \{1,2,\dots,m\}\\
        \text{and}\\
        x_j \geq 0, \;\;\; \forall j \in \{1,2,\dots,n\}
    \end{multline*}

    Dual Problem \begin{multline*}
        \text{minimize } W = \sum_{i=1}^m b_iy_i \\
        \text{subject to }\\
        \sum_{i=1}^m a_{ij}y_i \leq c_j, \;\;\; \forall j \in \{1,2,\dots,m\}\\
        \text{and}\\
        y_i \geq 0, \;\;\; \forall i \in \{1,2,\dots,n\}
    \end{multline*}
\end{multicols}

\subsubsection{Properties}
\begin{itemize}
    \item \textbf{Weak duality}: If $x$ is a feasible solution for the (P) and $y$ is a feasible solution for (D), then
    \[ cx \leq yb \]
    \item \textbf{Strong duality}: If $x^*$ is an optimal solution for (P) and $y^*$ is an optimal solution for (D), then,
    \[ cx^* = y^*b \]
\end{itemize}
Thus, these two properties imply that $cx < yb$ for feasible solutions if one or both of them
are not optimal for their respective problems, whereas equality holds when both are optimal.

\begin{itemize}
    \item \textbf{Symmetry property}: For any primal problem and its dual problem, all relationships between them must be 
    symmetric because the dual of this dual problem is this primal problem.
\end{itemize}

\subsubsection{Relation between primal and dual problem}
The following are the only possible relationships between the two problems.
\begin{enumerate}
     \item If one problem has feasible solutions and a bounded objective function (and
     so has an optimal solution), then so does the other problem, so both the weak
     and strong duality properties are applicable.
     \item If one problem has feasible solutions and an unbounded objective function
     (and so no optimal solution), then the other problem has no feasible solutions.
     \item If one problem has no feasible solutions, then the other problem has either no
     feasible solutions or an unbounded objective function.
\end{enumerate}

\subsubsection{Complementary Basic Solutions}
Because the problem (D) is also a LO problem, it also has a corner point solution. Ussing the augmented form of the problem 
we can express these corner-point solution as a basic solution. Because the functional constrains have $\geq$, this solution
is obtained by subtracting the surplus (rather than adding the slack) from the Left-HS of each constraint $j$. Thus,
\[ z_j - c_j = \sum_{i=1}^m a_{ij}y_i - c_j,\;\; \forall j \in \{1,2,\dots,n\} \]
So $z_j-c_j$ is the \textbf{}{surplus variable} for constraint $j$ (or its slack variable if
the constraint is multiplied through by -1). \\

One of the important relationships between the primal and dual problems is a direct cor-
respondence between their basic solutions.
\begin{itemize}
    \item \textbf{Complementary basic solutions property}: Each basic solution in the primal
    problem has a \textbf{complementary basic solution} in the dual problem, where their
    respective objective function values (Z and W) are equal.
    \item \textbf{Complementary slackness property}: CHECK THIS and addapt to the defition given here
    \[ (\sum_{i=1}^m a_{ij}\pi_i - r_j) \bar{x}_j = 0, \;\; \forall j \in \{ 1,2,\dots,n \} \]
\end{itemize}


\subsection{Sensitivity Analysis}
Once we already have the optimal solution for a LO problem, it would be important to analyze if slight changes in the constraints or the objective function
affects the optimal solution or it remains the same. Specifically, we would want to analyse and find the \textbf{parameter bounds} of the optimal solution.
We are going to analyze two types of changes:
\begin{enumerate}
    \item Changes in objective coefficients
    \item Changes in the right-hand side of the constraints    
\end{enumerate}

\subsubsection{Changes in objective coefficients}
\textbf{Simplified example.}\\
Given an objective function (o.f.),
\[ \text{maximize } z(x_1, x_2) = 5x_1 + 4x_2 \]
Subject to
\[ \text{M1: } 6x_1 + 4x_2 \leq 24 \]
\[\text{M2: } x_1 + 2x_2 \leq 6 \]
with optimal solution in $x^* = (3, 3/2)$, we can perform the sensitivity analysis if the changes in the o.f. are expressed as,
\[ \text{maximize } \hat{z}(x_1, x_2) = (5+\Delta r_1)x_1 + (4 + \Delta r_2)x_2 \]

\textbf{Solution:}\\
We know that $x^* = (3, 3/2)$ for $(\Delta r_1, \Delta r2) = 0$, but, for which increments of $(\Delta r_1, \Delta r2)$ is optimal $x^*$?
$x^*$ is optimal only and only if
\begin{center}
    slope of M1 constraint $\leq$ slope of $\hat{z} \leq$ slope of M2 constraint
\end{center} 
\[ -\frac{3}{2} \leq \frac{5 + \Delta r_1}{4 + \Delta r_2} \leq -\frac{1}{2} \]
Note: We can draw the region of solution if we represent the increments as axis (include image)

\subsubsection{Changes in the RHS of the constraints}
Simplified example.\\
Given constraints,\\
M1: $ 6x_1 + 4x_2 = 24 $ \\
M2: $ x_1 + 2x_2 = 6 $\\
We can repeat the same approach\\
M1: $ 6+x_1 + 4x_2 = 24 + \Delta b_1 $\\
M2: $ x_1 + 2x_2 = 6 + \Delta b_2 $

\textbf{Shadow prize and range of validity}
Shadow price is change of the objective function per unit in the RHS\\
Range of validity is the range of values of the RHS in which the solution remains optimal
